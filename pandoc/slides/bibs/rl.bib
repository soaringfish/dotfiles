@article{Berral2010,
abstract = {As energy-related costs have become a major economical factor for IT infrastructures and data-centers, companies and the research community are being challenged to find better and more efficient power-aware resource management strategies. There is a growing interest in "Green" IT and there is still a big gap in this area to be covered.},
annote = {Linear Regression; M5P},
author = {Berral, Josep Ll and Goiri, {\'{I}}{\铨殓犷物醅裔睇堙稞犷术扉茑猃棋蝌犷犷酋轸狎衄曙蜾犷轻鲠熹茑猃议汜蜾犷燥蝌弩曙蜾辇滹卑北吹狈贡潮串狈贡炒过骈戾鸿镯瀵沌醑祜汜殳箬狎瀵溽翎湾钿屐妁挑洚湾钿屐妁腻箅麸鸠娘黝祜徜邃洛蝌犰弭犰舶卑燥麽蜾孱弪琦狩狎筱桢漉扉铉轭溽翎沐铘弪躞轭磲汨轭戾狎铋铉ú┊痄婧痄纨轶忸狗腑杯吹俺鞍床饼觑躜钺序镢邋溟铉镦翳斌深翦蝾狒轱钺蔑铈弪孱沐镱蓬弪琦沛骈汩孱蔑眇豸轭犷五赭矧腴铉迮铄蜱卑脲黠蜾溽翎沐铘弪磲汨轭戾狎铋铉痫麇彐骈汩孱泫筱桢漉扉铉疳珏脖谍糸綮燥麽蜾孱弪琦狩狎筱桢漉扉铉轭溽翎沐铘弪躞轭磲汨轭戾狎铋铉躜梏麴函痫螋犰徙懋矧绡汩翎糸镱沔砜滹殇奖饭背贝狈贡炒过鲲祯礤昌遽舶卑泪螋殂戾棋滹蝻鲠舶岸狨翳矧棋滹蝻鲠领屮犷潋犷渝祠弪歪蜱犷禹轸璎烷汨徨湍骈戾鸿镯瀵沌醑祜汜殳箬狎瀵溽翎湾钿屐妁挑洚湾钿屐妁腻箅麸鸠娘黝祜徜邃棋滹蝻鲠渝祠弪禹轸舶岸冕汨瀛驷轵翳蝈徜筱桢漉扉铉骘眭祠殂矧痱镢弩箫蝮痄婧痄纨觑躜钺拈鲩箝镱镦蓬玳铄弪轭犷莒滹趔疳珏杯辈糸綮冕汨瀛驷轵翳蝈徜筱桢漉扉铉骘眭祠殂矧痱镢弩箫蝮躜梏麴函鼢鳟邋泱栳蝣狎洚邃醑fedorova/papers/cache-fair.pdf},
year = {2006}
}
@article{Fedorova2007,
abstract = {We make a case that a thread scheduler for heterogeneous multicore systems should target three objectives: optimal performance, core assignment balance and response time fairness. Performance optimization via optimal thread-to-core assignment has been explored in the past; in this paper we demonstrate the need for balanced core assignment. We show that unbalanced core assignment results in completion time jitter and inconsistent priority enforcement; we then present a simple fix to the Linux scheduler that eliminates these problems. The second part of the paper addresses the problem of building the HMC scheduler that balances all three objectives. This is a difficult optimization problem. We introduce a definition of this scheduling problem in terms of these three objectives and present a blueprint for a self-tuning algorithm based on reinforcement learning that maximizes a performance function that is an arbitrary weighted sum of these three objectives. Implementing and evaluating this algorithm is the subject of our future work.},
author = {Fedorova, Alexandra and Vengerov, David and Doucette, Daniel},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fedorova, Vengerov, Doucette - 2007 - Operating System Scheduling On Heterogeneous Core Systems.pdf:pdf},
journal = {Proc. of OSHMA workshop, 16th PACT},
title = {{Operating System Scheduling On Heterogeneous Core Systems}},
year = {2007}
}
@article{Glaubius2009,
author = {Glaubius, Robert and Tidwell, Terry and Gill, Christopher D and Smart, William D},
doi = {10.1504/IJAACS.2009.026786},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glaubius et al. - 2009 - Scheduling policy design for autonomic systems.pdf:pdf},
issn = {1754-8632},
journal = {International Journal of Autonomous and Adaptive Communications Systems},
keywords = {autonomic systems,policy iteration,scheduling,state space},
number = {3},
pages = {276},
title = {{Scheduling policy design for autonomic systems}},
url = {http://www.inderscience.com/link.php?id=26786},
volume = {2},
year = {2009}
}
@article{Glaubius2010,
author = {Glaubius, Robert and Tidwell, Terry and Gill, Christopher and Smart, William D},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glaubius et al. - 2010 - Real-Time Scheduling via Reinforcement Learning.pdf:pdf},
isbn = {9780974903965},
journal = {Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI)},
title = {{Real-Time Scheduling via Reinforcement Learning}},
year = {2010}
}
@article{Glaubius,
author = {Glaubius, Robert and Tidwell, Terry and Smart, William D and Gill, Christopher},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glaubius et al. - Unknown - Scheduling Design and Verification for Open Soft Real-time Systems.pdf:pdf},
title = {{Scheduling Design and Verification for Open Soft Real-time Systems}}
}
@article{Ipek2008,
author = {Ipek, Engin and Mutlu, Onur and Martinez, Jose F and Caruana, Rich},
doi = {10.1109/ISCA.2008.21},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ipek et al. - 2008 - Self-Optimizing Memory Controllers A Reinforcement Learning Approach.pdf:pdf},
isbn = {9780769531748},
issn = {10636897},
journal = {Computer Architecture, International Symposium on},
number = {June},
pages = {39--50},
title = {{Self-Optimizing Memory Controllers: A Reinforcement Learning Approach}},
volume = {0},
year = {2008}
}
@article{Jensen1985,
abstract = {Process scheduling in real-time systems has almost invariably used one or more of three algorithms: fixed priority, FIFO, or round robin. The reasons for these choices are simplicity and speed in the operating system, but the cost to the system in terms of reliability and maintainability have not generally been assessed. This paper originates from the notion that the primary distinguishing characteristic of a real-time system is the concept that completion of a process or a set of processes has a value to the system which can be expressed as a function of time. This notion is described in terms of a time-driven scheduling model for real-time operating systems and provides a tool for measuring the effectiveness of most of the currently used process schedulers in real-time systems. Applying this model, we have constructed a multiprocessor real-time system simulator with which we measure a number of well-known scheduling algorithms such as Shortest Process Time (SPT), Deadline, Shortest Slack Time, FIFO, and a fixed priority scheduler, with respect to the resulting total system values. This approach to measuring the process scheduling effectiveness is a first step in our longer term effort to produce a scheduler which will explicitly schedule real-time processes in such a way that their execution times maximize their collective value to the system, either in a shared memory multiprocessing environment or in multiple nodes of a distributed processing environment.},
author = {Jensen, E. Douglas and Locke, C. Douglas and Tokuda, Hideyuki},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jensen, Locke, Tokuda - 1985 - A Time-Driven Scheduling Model for Real-Time Operating Systems.pdf:pdf},
journal = {6th IEEE Real-Time Systems Symposium RTSS' 85},
pages = {112--122},
title = {{A Time-Driven Scheduling Model for Real-Time Operating Systems}},
year = {1985}
}
@article{Mart\\inez2011,
author = {Mart$\backslash$'$\backslash$inez, Y. and Now{\'{e}}, a. and Su{\'{a}}rez, J. and Bello, R.},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mart'inez et al. - 2011 - A Reinforcement Learning Approach for the Flexible Job Shop Scheduling Problem(2).pdf:pdf},
journal = {Learning and Intelligent Optimization},
pages = {253--262},
title = {{A Reinforcement Learning Approach for the Flexible Job Shop Scheduling Problem}},
url = {http://www.springerlink.com/index/W8504H7312RG3G06.pdf},
year = {2011}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Naim,
author = {Naim, Iftekhar},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naim - Unknown - Reinforcement Learning for Operating Systems Scheduling Introduction {\&} Motivation Resource scheduling critical.pdf:pdf},
title = {{Reinforcement Learning for Operating Systems Scheduling Introduction {\&} Motivation Resource scheduling : critical}}
}
@article{Ravindran2005,
author = {Ravindran, Binoy and Jensen, E Douglas and Li, Peng},
doi = {10.1109/ISORC.2005.39},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravindran, Jensen, Li - 2005 - On recent advances in timeutility function real-time scheduling and resource management.pdf:pdf},
isbn = {0769523560},
journal = {Object-Oriented Real-Time Distributed Computing, 2005. ISORC 2005. Eighth IEEE International Symposium on},
pages = {55--60},
title = {{On recent advances in time/utility function real-time scheduling and resource management}},
year = {2005}
}
@article{Silver,
author = {Silver, David and Deepmind, Google},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver, Deepmind - Unknown - Deep Reinforcement Learning Reinforcement Learning AI = RL.pdf:pdf},
title = {{Deep Reinforcement Learning Reinforcement Learning : AI = RL}}
}
@article{Srinivasan2003,
abstract = { In this paper, we consider fair scheduling of soft real-time applications on multiprocessors using the earliest pseudo deadline first (EPDF) Pfair algorithm. Our main contributions are twofold. First, we establish a condition for ensuring a tardiness of at most one quantum under EPDF. This condition is very liberal and should often hold in practice. Second, we present simulation results involving randomly-generated task sets, including those that do not satisfy our condition. In these experiments, deadline misses were rare, and no misses by more than one quantum ever occurred.},
author = {Srinivasan, a. and Anderson, J.H.},
doi = {10.1109/EMRTS.2003.1212727},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivasan, Anderson - 2003 - Efficient scheduling of soft real-time applications on multiprocessors.pdf:pdf},
isbn = {0-7695-1936-9},
issn = {10683070},
journal = {15th Euromicro Conference on Real-Time Systems, 2003. Proceedings.},
title = {{Efficient scheduling of soft real-time applications on multiprocessors}},
year = {2003}
}
@article{Taylor2004,
author = {Taylor, Publisher},
doi = {10.1080/07408170490278698},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor - 2004 - A reinforcement learning approach to stochastic business games.pdf:pdf},
number = {908841869},
title = {{A reinforcement learning approach to stochastic business games}},
year = {2004}
}
@article{Tidwell2010,
author = {Tidwell, Authors Terry and Glaubius, Robert and Gill, Christopher D and Smart, William D and Tidwell, Terry},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tidwell et al. - 2010 - Systems Optimal Time Utility Based Scheduling Policy Design for Cyber-Physical Systems.pdf:pdf},
journal = {Engineering},
number = {314},
title = {{Systems Optimal Time Utility Based Scheduling Policy Design for Cyber-Physical Systems}},
volume = {27},
year = {2010}
}
@article{Tidwell2010a,
abstract = {Classical scheduling abstractions such as deadlines and priorities do not readily capture the complex timing semantics found in many real-time cyber-physical systems. Time utility functions provide a necessarily richer description of timing semantics, but designing utility-aware scheduling policies using them is an open research problem. In particular, scheduling design that optimizes expected utility accrual is needed for real-time cyber-physical domains. In this paper we design scheduling policies that optimize expected utility accrual for cyber-physical systems with periodic, non-preemptable tasks that run with stochastic duration. These policies are derived by solving a Markov Decision Process formulation of the scheduling problem. We use this formulation to demonstrate that our technique improves on existing heuristic utility accrual scheduling policies.},
author = {Tidwell, Terry and Glaubius, Robert and Gill, Christopher D. and Smart, William D.},
doi = {10.1109/RTSS.2010.28},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tidwell et al. - 2010 - Optimizing expected time utility in cyber-physical systems schedulers.pdf:pdf},
isbn = {9780769542980},
issn = {10528725},
journal = {Proceedings - Real-Time Systems Symposium},
pages = {193--201},
title = {{Optimizing expected time utility in cyber-physical systems schedulers}},
year = {2010}
}
@article{Tidwell2008,
author = {Tidwell, Terry and Glaubius, Robert and Gill, Christopher and Smart, William D.},
doi = {10.1007/978-3-540-69295-9_14},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tidwell et al. - 2008 - Scheduling for reliable execution in autonomic systems.pdf:pdf},
isbn = {3540692940},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {149--161},
title = {{Scheduling for reliable execution in autonomic systems}},
volume = {5060 LNCS},
year = {2008}
}
@article{Vengerov2007,
abstract = {This paper presents a general framework for performing adaptive reconfiguration of a distributed system based on maximizing the long-term business value, defined as the discounted sum of all future rewards and penalties. The problem of dynamic resource allocation among multiple entities sharing a common set of resources is used as an example. A specific architecture (DRA-FRL) is presented, which uses the emerging methodology of reinforcement learning in conjunction with fuzzy rulebases to achieve the desired objective. This architecture can work in the context of existing resource allocation policies and learn the values of the states that the system encounters under these policies. Once the learning process begins to converge, the user can allow the DRA-FRL architecture to make some additional resource allocation decisions or override the ones suggested by the existing policies so as to improve the long-term business value of the system. The DRA-FRL architecture can also be deployed in an environment without any existing resource allocation policies. An implementation of the DRA-FRL architecture in Solaris 10 demonstrated a robust performance improvement in the problem of dynamically migrating CPUs and memory blocks between three resource partitions so as to match the stochastically changing workload in each partition, both in the presence and in the absence of resource migration costs. (c) 2006 Elsevier Ltd. All rights reserved.},
author = {Vengerov, D},
doi = {Doi 10.1016/J.Engappai.2006.06.019},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vengerov - 2007 - A reinforcement learning approach to dynamic resource allocation.pdf:pdf},
isbn = {0952-1976},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {reinforcement learning,resource allocation,utility computing},
pages = {1--18},
title = {{A reinforcement learning approach to dynamic resource allocation}},
year = {2007}
}
@article{Vengerov2009,
abstract = {This paper presents a general methodology for online scheduling of parallel jobs onto multi-processor servers in a soft real-time environment, where the final utility of each job decreases with the job completion time. A solution approach is presented where each server uses Reinforcement Learning for tuning its own value function, which predicts the average future utility per time step obtained from completed jobs based on the dynamically observed state information. The server then selects jobs from its job queue, possibly preempting some currently running jobs and "squeezing" some jobs into fewer CPUs than they ideally require to maximize the value of the resulting server state. The experimental results demonstrate the feasibility and benefits of the proposed approach. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Vengerov, David},
doi = {10.1016/j.future.2008.02.006},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vengerov - 2009 - A reinforcement learning framework for utility-based scheduling in resource-constrained systems.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Job scheduling,Reinforcement learning,Time utility functions,Utility computing,Utilization},
number = {7},
pages = {728--736},
publisher = {Elsevier B.V.},
title = {{A reinforcement learning framework for utility-based scheduling in resource-constrained systems}},
url = {http://dx.doi.org/10.1016/j.future.2008.02.006},
volume = {25},
year = {2009}
}
@article{With,
author = {With, Introduction and Learning, Reinforcement},
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/With, Learning - Unknown - Reinforcement Learning for Scheduling Threads on a Multi-Core Processor.pdf:pdf},
title = {{Reinforcement Learning for Scheduling Threads on a Multi-Core Processor}}
}
@misc{,
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - tr-rl-robot-navigation.gz:gz},
title = {tr-rl-robot-navigation}
}
@misc{,
file = {:home/cgu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 10.ps:ps},
title = {10}
}
